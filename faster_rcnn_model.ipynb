{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport os\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets, transforms\nfrom PIL import Image\nfrom xml.dom.minidom import parse\ntorch.manual_seed(42)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-31T11:53:39.594365Z","iopub.execute_input":"2021-10-31T11:53:39.59479Z","iopub.status.idle":"2021-10-31T11:53:39.603888Z","shell.execute_reply.started":"2021-10-31T11:53:39.594745Z","shell.execute_reply":"2021-10-31T11:53:39.60264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pycocotools","metadata":{"execution":{"iopub.status.busy":"2021-10-31T11:53:39.605709Z","iopub.execute_input":"2021-10-31T11:53:39.606458Z","iopub.status.idle":"2021-10-31T11:53:55.594567Z","shell.execute_reply.started":"2021-10-31T11:53:39.606414Z","shell.execute_reply":"2021-10-31T11:53:55.59372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"../input/scripts1/engine.py\", dst = \"../working/engine.py\")\ncopyfile(src = \"../input/scripts1/utils.py\", dst = \"../working/utils.py\")\ncopyfile(src = \"../input/scripts1/transforms.py\", dst = \"../working/transforms.py\")\ncopyfile(src = \"../input/scripts1/coco_utils.py\", dst = \"../working/coco_utils.py\")\ncopyfile(src = \"../input/scripts1/coco_eval.py\", dst = \"../working/coco_eval.py\")\n\n# import all our functions\nfrom engine import *\nimport transforms as T\nimport utils","metadata":{"execution":{"iopub.status.busy":"2021-10-31T11:53:55.596993Z","iopub.execute_input":"2021-10-31T11:53:55.597281Z","iopub.status.idle":"2021-10-31T11:53:55.630076Z","shell.execute_reply.started":"2021-10-31T11:53:55.59724Z","shell.execute_reply":"2021-10-31T11:53:55.629366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport sys\nimport time\nimport torch\n\nimport torchvision.models.detection.mask_rcnn\n\nfrom coco_utils import get_coco_api_from_dataset\nfrom coco_eval import CocoEvaluator\nimport utils\n\ndef train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n    model.train()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    header = 'Epoch: [{}]'.format(epoch)\n\n    lr_scheduler = None\n    if epoch == 0:\n        warmup_factor = 1. / 1000\n        warmup_iters = min(1000, len(data_loader) - 1)\n\n        lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n\n    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n\n        losses = sum(loss for loss in loss_dict.values())\n\n        # reduce losses over all GPUs for logging purposes\n        loss_dict_reduced = utils.reduce_dict(loss_dict)\n        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n\n        loss_value = losses_reduced.item()\n\n        if not math.isfinite(loss_value):\n            print(\"Loss is {}, stopping training\".format(loss_value))\n            print(loss_dict_reduced)\n            sys.exit(1)\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        if lr_scheduler is not None:\n            lr_scheduler.step()\n\n        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n  \ndef _get_iou_types(model):\n    model_without_ddp = model\n    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n        model_without_ddp = model.module\n    iou_types = [\"bbox\"]\n    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n        iou_types.append(\"segm\")\n    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n        iou_types.append(\"keypoints\")\n    return iou_types\n\n@torch.no_grad()\ndef evaluate(model, data_loader, device):\n    n_threads = torch.get_num_threads()\n    # FIXME remove this and make paste_masks_in_image run on the GPU\n    torch.set_num_threads(1)\n    cpu_device = torch.device(\"cpu\")\n    model.eval()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    header = 'Test:'\n\n    coco = get_coco_api_from_dataset(data_loader.dataset)\n    iou_types = _get_iou_types(model)\n    coco_evaluator = CocoEvaluator(coco, iou_types)\n\n    for image, targets in metric_logger.log_every(data_loader, 100, header):\n        image = list(img.to(device) for img in image)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        torch.cuda.synchronize()\n        model_time = time.time()\n        outputs = model(image)\n\n        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n        model_time = time.time() - model_time\n\n        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n        evaluator_time = time.time()\n        coco_evaluator.update(res)\n        evaluator_time = time.time() - evaluator_time\n        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n\n    # gather the stats from all processes\n    metric_logger.synchronize_between_processes()\n    print(\"Averaged stats:\", metric_logger)\n    coco_evaluator.synchronize_between_processes()\n\n    # accumulate predictions from all images\n    coco_evaluator.accumulate()\n    coco_evaluator.summarize()\n    torch.set_num_threads(n_threads)\n    return coco_evaluator\n","metadata":{"execution":{"iopub.status.busy":"2021-10-31T11:53:55.631375Z","iopub.execute_input":"2021-10-31T11:53:55.631797Z","iopub.status.idle":"2021-10-31T11:53:55.653325Z","shell.execute_reply.started":"2021-10-31T11:53:55.63176Z","shell.execute_reply":"2021-10-31T11:53:55.652636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MarkDataset(torch.utils.data.Dataset):\n    def __init__(self, root, transforms=None):\n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to ensure that they are aligned\n        self.imgs = list(sorted(os.listdir(os.path.join(root, \"train/images\"))))\n        self.bbox_xml = list(sorted(os.listdir(os.path.join(root, \"train/annotations/PASCAL_VOC_xml\"))))\n \n    def __getitem__(self, idx):\n        # load images and bbox\n        img_path = os.path.join(self.root, \"train/images\", self.imgs[idx])\n        bbox_xml_path = os.path.join(self.root, \"train/annotations/PASCAL_VOC_xml\", self.bbox_xml[idx])\n        img = Image.open(img_path).convert(\"RGB\")        \n        \n        # Read file, VOC format dataset label is xml format file\n        dom = parse(bbox_xml_path)\n        # Get Document Element Object\n        data = dom.documentElement\n        # Get objects\n        objects = data.getElementsByTagName('object')        \n        # get bounding box coordinates\n        boxes = []\n        labels = []\n        for object_ in objects:\n            # Get the contents of the label\n            name = object_.getElementsByTagName('name')[0].childNodes[0].nodeValue  # Is label, mark_type_1 or mark_type_2\n            \n            # labels.append(np.int(name[-1]))  # Background label is 0, mark_type_1 and mark_type_2 labels are 1 and 2, respectively\n            \n            bndbox = object_.getElementsByTagName('bndbox')[0]\n            xmin = int(np.float(bndbox.getElementsByTagName('xmin')[0].childNodes[0].nodeValue))\n            ymin = int(np.float(bndbox.getElementsByTagName('ymin')[0].childNodes[0].nodeValue))\n            xmax = int(np.float(bndbox.getElementsByTagName('xmax')[0].childNodes[0].nodeValue))\n            ymax = int(np.float(bndbox.getElementsByTagName('ymax')[0].childNodes[0].nodeValue))\n            boxes.append([xmin, ymin, xmax, ymax]) \n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        # there is only one class\n        labels = torch.ones((len(boxes),), dtype=torch.int64)\n        # labels = torch.as_tensor(labels, dtype=torch.int64)        \n \n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((len(objects),), dtype=torch.int64)\n \n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        # Since you are training a target detection network, there is no target [masks] = masks in the tutorial\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n \n        if self.transforms is not None:\n            # Note that target (including bbox) is also transformed\\enhanced here, which is different from transforms from torchvision import\n            # Https://github.com/pytorch/vision/tree/master/references/detectionOfTransforms.pyThere are examples of target transformations when RandomHorizontalFlip\n            img, target = self.transforms(img, target)\n \n        return img, target\n \n    def __len__(self):\n        return len(self.imgs)\n\n# df = MarkDataset(r'../input/evraz-data/data_task2/')\n# df[50]\n# (<PIL.Image.Image image mode=RGB size=1920x1080 at 0x7FA8341A4F10>,\n#  {'boxes': tensor([[ 888.,  485., 1059., 1011.],\n#           [ 742.,  507., 1018., 1049.]]),\n#   'labels': tensor([1, 1]),\n#   'image_id': tensor([50]),\n#   'area': tensor([ 89946., 149592.]),\n#   'iscrowd': tensor([0, 0])})","metadata":{"execution":{"iopub.status.busy":"2021-10-31T11:53:55.656617Z","iopub.execute_input":"2021-10-31T11:53:55.656837Z","iopub.status.idle":"2021-10-31T11:53:55.674387Z","shell.execute_reply.started":"2021-10-31T11:53:55.656794Z","shell.execute_reply":"2021-10-31T11:53:55.673643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n \n      \ndef get_object_detection_model(num_classes):\n    # load an object detection model pre-trained on COCO\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n#     model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)\n#     model = torchvision.models.detection.n(pretrained=True)\n    \n    num_classes = 2 \n \n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n \n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-10-31T11:53:55.675725Z","iopub.execute_input":"2021-10-31T11:53:55.676213Z","iopub.status.idle":"2021-10-31T11:53:55.687792Z","shell.execute_reply.started":"2021-10-31T11:53:55.676178Z","shell.execute_reply":"2021-10-31T11:53:55.686821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import utils\nimport transforms as T\n \ndef get_transform(train):\n    transforms = []\n    transforms.append(T.ToTensor())\n    if train:\n        transforms.append(T.RandomHorizontalFlip(0.5))\n \n    return T.Compose(transforms)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T11:53:55.689159Z","iopub.execute_input":"2021-10-31T11:53:55.689642Z","iopub.status.idle":"2021-10-31T11:53:55.701694Z","shell.execute_reply.started":"2021-10-31T11:53:55.689607Z","shell.execute_reply":"2021-10-31T11:53:55.700874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root = r'../input/evraz-data/data_task2/'\nimport os\ndataset = MarkDataset(root, get_transform(train=True))\ndataset_test = MarkDataset(root, get_transform(train=False))\n\n# split the dataset in train and test set\ntorch.manual_seed(42)\nindices = torch.randperm(len(dataset)).tolist()\ndataset = torch.utils.data.Subset(dataset, indices[:-50])\ndataset_valid = torch.utils.data.Subset(dataset_test, indices[-50:])\n\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=4, shuffle=True, num_workers=2,\n    collate_fn=utils.collate_fn)\n\ndata_loader_valid = torch.utils.data.DataLoader(\n    dataset_valid, batch_size=2, shuffle=False, num_workers=2,\n    collate_fn=utils.collate_fn)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T11:53:55.703057Z","iopub.execute_input":"2021-10-31T11:53:55.703547Z","iopub.status.idle":"2021-10-31T11:53:55.817639Z","shell.execute_reply.started":"2021-10-31T11:53:55.703509Z","shell.execute_reply":"2021-10-31T11:53:55.816875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-10-31T11:53:55.819578Z","iopub.execute_input":"2021-10-31T11:53:55.820033Z","iopub.status.idle":"2021-10-31T11:53:55.82544Z","shell.execute_reply.started":"2021-10-31T11:53:55.819997Z","shell.execute_reply":"2021-10-31T11:53:55.82463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# our dataset has two classes only - background and person\nnum_classes = 2\n\nmodel = get_object_detection_model(num_classes)\n\nmodel.to(device)\n\n\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.0005,\n                            momentum=0.9, weight_decay=0.0005)\n\n\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=5,\n                                               gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T11:53:55.827081Z","iopub.execute_input":"2021-10-31T11:53:55.82765Z","iopub.status.idle":"2021-10-31T11:54:05.502205Z","shell.execute_reply.started":"2021-10-31T11:53:55.82761Z","shell.execute_reply":"2021-10-31T11:54:05.50146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 2\nimport math\nfor epoch in range(num_epochs):\n    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=50)\n    \n    lr_scheduler.step()\n    \n    evaluate(model, data_loader_valid, device=device)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T11:54:05.5034Z","iopub.execute_input":"2021-10-31T11:54:05.503678Z","iopub.status.idle":"2021-10-31T11:57:55.940508Z","shell.execute_reply.started":"2021-10-31T11:54:05.50364Z","shell.execute_reply":"2021-10-31T11:57:55.939662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model, f'model_epoch_{num_epochs}.pkl')","metadata":{"execution":{"iopub.status.busy":"2021-10-31T11:57:55.94231Z","iopub.execute_input":"2021-10-31T11:57:55.942611Z","iopub.status.idle":"2021-10-31T11:57:56.286047Z","shell.execute_reply.started":"2021-10-31T11:57:55.942552Z","shell.execute_reply":"2021-10-31T11:57:56.285305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict","metadata":{}},{"cell_type":"code","source":"class TestDataset(torch.utils.data.Dataset):\n    def __init__(self, root, transforms=None):\n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to ensure that they are aligned\n        self.imgs = list(sorted(os.listdir(os.path.join(root, \"test/images\"))))\n        # self.bbox_xml = list(sorted(os.listdir(os.path.join(root, \"train/annotations/PASCAL_VOC_xml\"))))\n \n    def __getitem__(self, idx):\n        # load images and bbox\n        image_idx = self.imgs[idx]\n        img_path = os.path.join(self.root, \"test/images\", self.imgs[idx])\n        # bbox_xml_path = os.path.join(self.root, \"train/annotations/PASCAL_VOC_xml\", self.bbox_xml[idx])\n        img = Image.open(img_path).convert(\"RGB\") \n\n        image_id = torch.tensor([idx])\n\n\n        target = {'labels': torch.as_tensor([[0]], dtype=torch.float32),\n                  'boxes': torch.as_tensor([[0, 0, 0, 0]], dtype=torch.float32)}\n\n        image_dict = {\n                    'image': img,\n                    'bboxes': target['boxes'],\n                    'labels': target['labels']\n                    }\n        # img = self.transforms(**image_dict)['image']  \n        img = self.transforms(img)        \n        return img, image_idx\n\n    def __len__(self):\n        return len(self.imgs)  ","metadata":{"execution":{"iopub.status.busy":"2021-10-31T11:57:56.304389Z","iopub.execute_input":"2021-10-31T11:57:56.305736Z","iopub.status.idle":"2021-10-31T11:57:56.314743Z","shell.execute_reply.started":"2021-10-31T11:57:56.305697Z","shell.execute_reply":"2021-10-31T11:57:56.313932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trans = transforms.Compose([transforms.ToTensor()])\nroot = r'../input/evraz-data/data_task2/'\ntest_dataset = TestDataset(root, transforms=trans)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2, shuffle=False)\n\ndetection_threshold = 0.5\nresults = []\ndevice = 'cuda'\n\nresults = []\nvalues = {}\n\nfor images, image_ids in test_loader:\n    images = list(image.to(device) for image in images)\n    outputs = model(images)\n    for i, image in enumerate(images):\n        boxes = outputs[i]['boxes'].data.cpu().numpy()\n        scores = outputs[i]['scores'].data.cpu().numpy()\n        boxes = boxes[scores >= 0.9].astype(np.int32)\n        scores = scores[scores >= detection_threshold]\n\n        image_id = image_ids[i]\n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]         \n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]  # #Convert the box formate to [Xmin,Ymin,W,H]\n\n        values[image_id] = {\n            'boxes' : boxes,\n            'score' :scores\n        }\n\n        result = {\n            'image_id' : image_id,\n            'boxes' : boxes,\n            'score' : scores\n        }\n        results.append(result)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-31T11:57:56.316153Z","iopub.execute_input":"2021-10-31T11:57:56.316414Z","iopub.status.idle":"2021-10-31T11:58:21.051952Z","shell.execute_reply.started":"2021-10-31T11:57:56.316381Z","shell.execute_reply":"2021-10-31T11:58:21.051082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sort_bboxes(x):\n    if x.shape[1] > 3:    \n        x = sorted(x, key=lambda z: (z[0], z[1], z[2], z[3]))\n        x = np.array(x)\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-10-31T11:58:21.053348Z","iopub.execute_input":"2021-10-31T11:58:21.05361Z","iopub.status.idle":"2021-10-31T11:58:21.059977Z","shell.execute_reply.started":"2021-10-31T11:58:21.053561Z","shell.execute_reply":"2021-10-31T11:58:21.059205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nwith open('../input/samplesubmit/submission_example.json') as train:\n    example = json.load(train)\n    \nfor i in range(len(example['images'])):\n    # id in images == image_id in annotations\n    file_name = example['images'][i]['file_name']\n    image_id = example['images'][i]['id']\n    box = values[file_name]['boxes']\n    box = sort_bboxes(box)\n    \n    # box = list(box[0])\n    score = values[file_name]['score']\n    score = list(score)\n    # print(image_id)\n\n    for j in range(len(example['annotations'])):\n        if example['annotations'][j]['image_id'] == image_id:\n            # print(example['annotations'][j]['bbox'])\n            for l in range(len(box)):\n                # print(box[l])\n                example['annotations'][j]['bbox'] = list(map(int, box[l]))\n            # print(example['annotations'][j]['bbox'])\n            \nwith open('submit10.json', 'w') as train:\n    json.dump(example, train)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T11:58:21.061154Z","iopub.execute_input":"2021-10-31T11:58:21.063305Z","iopub.status.idle":"2021-10-31T11:58:21.108133Z","shell.execute_reply.started":"2021-10-31T11:58:21.063273Z","shell.execute_reply":"2021-10-31T11:58:21.107459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}